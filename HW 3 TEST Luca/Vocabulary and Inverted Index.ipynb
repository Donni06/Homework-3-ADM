{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse URL's from html 3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "# Preprocessing libraries\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dic, file): #defining a function to save files right away\n",
    "    f = open('{}.txt'.format(file), 'w',encoding=\"utf8\") #open a new file in write mode (empty)\n",
    "    f.write(str(dic)) #write in it what we need (it will always be a dictionary, hence 'dic')\n",
    "    f.close() #closing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentlist = {} #to keep track of the words in the all the docs --> {doc_i : ['house, 'garden'.....]}\n",
    "vocabulary = {} #to keep track of all the pre-processed terms and their ids (term_ids) --> {'house':1, 'garden':2}\n",
    "inverted_index = {} #this is the inv idx. term_id as the key and name of the documents as a list of their values: {1: [doc_1, doc_2, ....], 2: [doc_2, doc_6, ....]}\n",
    "indexOfWord = 0 #this is used to give the id to the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the paths for the tsv documents in the directory\n",
    "docpaths = [f for f in listdir(r\"C:\\Users\\Luca\\Desktop\\-\\Università\\Magistrale\\Primo anno\\Primo semestre\\ADM\\Homeworks\\HW 3 TEST\\TSV\") if isfile(join(r\"C:\\Users\\Luca\\Desktop\\-\\Università\\Magistrale\\Primo anno\\Primo semestre\\ADM\\Homeworks\\HW 3 TEST\\TSV\", f))] \n",
    "\n",
    "for doc in docpaths: \n",
    "    # open the tsv file in read mode using 'r'\n",
    "    with open('TSV\\{}'.format(doc), 'r',encoding=\"utf8\") as file: \n",
    "        \n",
    "        # Creating a dataframe for each movie\n",
    "        df = pd.read_csv(file, sep = '\\t', encoding='utf8')\n",
    "        \n",
    "        # transforming Intro and Plot from Series to string\n",
    "        intro = str(df['Intro'][0])\n",
    "        plot = str(df['Plot'][0])\n",
    "        col = []\n",
    "        \n",
    "        # Step 1 INTRO\n",
    "        document1 = intro \n",
    "        col.append(document1.replace('\\n',' '))\n",
    "        \n",
    "        # Step 1 PLOT\n",
    "        document2 = plot \n",
    "        col.append(document2.replace('\\n',' '))\n",
    "        \n",
    "        # Step 2\n",
    "        # Taking Intro and Plot to tokenize\n",
    "        mytext = col[0] + col[1] \n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(mytext) \n",
    "        # Removing stopwords, special characters, stemming\n",
    "        filtered_words = [nltk.stem.PorterStemmer().stem(word) for word in tokens if word not in nltk.corpus.stopwords.words('english') and word not in \".,'()\"]\n",
    "        \n",
    "        # Step 3\n",
    "        #creating a document list: for every document we will have the tokenized words \n",
    "        documentlist[doc] = filtered_words \n",
    "        \n",
    "        for w in filtered_words: \n",
    "            # Creating the vocabulary\n",
    "            if w not in vocabulary: \n",
    "                word_index += 1    \n",
    "                vocabulary[w] = word_index  \n",
    "\n",
    "            # Creating the inverted index\n",
    "            if vocabulary[w] not in inverted_index: \n",
    "                temp = []  # Creating a list so we can append the documents that have that word\n",
    "                temp.append(doc)  #appending the first doc\n",
    "                inverted_index[vocabulary[w]] = temp #creating the index key and giving them the value\n",
    "\n",
    "            elif doc not in inverted_index[vocabulary[w]]: #if the key was already created\n",
    "                inverted_index[vocabulary[w]].append(doc)  #simply append the doc name\n",
    "                break\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_file(inverted_index,\"inverted_index\")\n",
    "save_dict_to_file(vocabulary,\"vocabulary\")\n",
    "save_dict_to_file(documentlist,\"documentlist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
