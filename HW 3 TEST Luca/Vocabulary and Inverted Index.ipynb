{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse URL's from html 3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "# Preprocessing libraries\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dic, file): #defining a function to save files right away\n",
    "    f = open('{}.txt'.format(file), 'w',encoding=\"utf8\") #open a new file in write mode (empty)\n",
    "    f.write(str(dic)) #write in it what we need (it will always be a dictionary, hence 'dic')\n",
    "    f.close() #closing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentlist = {} #to keep track of the words in the all the docs --> {doc_i : ['house, 'garden'.....]}\n",
    "vocabulary = {} #to keep track of all the pre-processed terms and their ids (term_ids) --> {'house':1, 'garden':2}\n",
    "inverted_index = {} #this is the inv idx. term_id as the key and name of the documents as a list of their values: {1: [doc_1, doc_2, ....], 2: [doc_2, doc_6, ....]}\n",
    "word_index = 0 #this is used to give the id to the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Luca\\Desktop\\-\\Universit√†\\Magistrale\\Primo anno\\Primo semestre\\ADM\\Homeworks\\Homework #3\\TSV' # use your path\n",
    "all_files = glob.glob(path + \"/*.tsv\")\n",
    "c = 0\n",
    "for filename in all_files:\n",
    "    \n",
    "    # Creating a dataframe for each movie\n",
    "    df = pd.read_csv(filename, sep='\\t', encoding  = 'utf-8')\n",
    "    doc = 'article_{}.tsv'.format(c)\n",
    "    \n",
    "    # Transforming Intro and Plot from Series to string\n",
    "    if pd.isnull(df['Intro'][0]):\n",
    "        try:\n",
    "            intro = str(df['Intro'][1])\n",
    "        except:\n",
    "            intro = ''\n",
    "            print(filename)\n",
    "    else :\n",
    "        intro = str(df['Intro'][0])\n",
    "\n",
    "    if pd.isnull(df['Plot'][0]):\n",
    "        try:\n",
    "            plot = str(df['Plot'][1])\n",
    "        except:\n",
    "            plot = ''\n",
    "            print(filename)\n",
    "    else :\n",
    "        plot = str(df['Plot'][0])\n",
    "\n",
    "    col = []\n",
    "\n",
    "    # Step 1 INTRO\n",
    "    document1 = intro #its a list of only one element, so we take that as a string\n",
    "    col.append(document1.replace('\\n',' '))\n",
    "\n",
    "    # Step 1 PLOT\n",
    "    document2 = plot #its a list of only one element, so we take that as a string\n",
    "    col.append(document2.replace('\\n',' '))\n",
    "\n",
    "    # Step 2 Taking Intro and Plot to tokenize\n",
    "    to_tokenize = col[0] + col[1] \n",
    "    tokens = nltk.word_tokenize(to_tokenize) #tokenization\n",
    "    filtered_words = [nltk.stem.PorterStemmer().stem(word) #removing stopwords, special characters, stemming\n",
    "                                for word in tokens if word not in nltk.corpus.stopwords.words('english') and word not in \".,'()\"]\n",
    "\n",
    "    # Step 3\n",
    "    # Creating a document list: for every document we will have the tokenized words \n",
    "    documentlist[doc] = filtered_words \n",
    "    for w in filtered_words: \n",
    "        \n",
    "        # VOCABULARY\n",
    "        if w not in vocabulary: \n",
    "            word_index += 1    \n",
    "            vocabulary[w] = word_index  \n",
    "        \n",
    "        # INVERTED INDEX\n",
    "        if vocabulary[w] not in inverted_index: \n",
    "            temp = [] \n",
    "            temp.append(doc)  \n",
    "            inverted_index[vocabulary[w]] = temp \n",
    "        \n",
    "        # If the key exists, append the document's name\n",
    "        elif doc not in inverted_index[vocabulary[w]]: \n",
    "            inverted_index[vocabulary[w]].append(doc)  \n",
    "\n",
    "\n",
    "    \n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_file(inverted_index,\"inverted_index\")\n",
    "save_dict_to_file(vocabulary,\"vocabulary\")\n",
    "save_dict_to_file(documentlist,\"documentlist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
