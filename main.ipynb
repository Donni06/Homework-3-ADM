{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import used \n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy \n",
    "import heapq\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Used for Output\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HTML pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon', 'https://en.wikipedia.org/wiki/The_Martyred_Presidents', 'https://en.wikipedia.org/wiki/Terrible_Teddy,_the_Grizzly_King', 'https://en.wikipedia.org/wiki/Jack_and_the_Beanstalk_(1902_film)', 'https://en.wikipedia.org/wiki/Alice_in_Wonderland_(1903_film)', 'https://en.wikipedia.org/wiki/The_Great_Train_Robbery_(1903_film)', 'https://en.wikipedia.org/wiki/The_Suburbanite', 'https://en.wikipedia.org/wiki/The_Little_Train_Robbery', 'https://en.wikipedia.org/wiki/The_Night_Before_Christmas_(1905_film)', 'https://en.wikipedia.org/wiki/Dream_of_a_Rarebit_Fiend_(1906_film)']\n"
     ]
    }
   ],
   "source": [
    "#import used\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "movies1 = BeautifulSoup(open(r\"moviespart1.html\"), \"html.parser\") #open all html files with BeautifulSoup from github saved pages\n",
    "\n",
    "movies1.prettify() #just to see all the html files\n",
    "\n",
    "urls = [] #create a list with all the urls in github pages\n",
    "for url in movies1.findAll('a', href=True):\n",
    "    urls.append(url['href'])\n",
    "    \n",
    "print(urls[0:10]) #print first 10 urls how example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We implemented a for loop instead of a function.####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a for loop to download all the url in urls list\n",
    "c = 0\n",
    "for url in urls:\n",
    "    \n",
    "    try :\n",
    "        urllib.request.urlretrieve(url, \"article{}.html\".format(c)) #Copy a network object denoted by a URL to a local file.\n",
    "        r = requests.get(url)\n",
    "        time.sleep(2)  #wait 2 seconds each request\n",
    "        c += 1\n",
    "        if r.status_code != 200: # used in case the request doesn't return code 200, that means an error has occured\n",
    "            raise Exception(\"Could not download URL\" + url)\n",
    "    except Exception:\n",
    "        time.sleep(1200) #in the case of the exception happens we stop the program for 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Html Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the functions used to get INTRO, PLOT and INFOBOX from the TSV files ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title (soup):\n",
    "    # We get the title of the movie\n",
    "    title = soup.select(\"#firstHeading\")[0].text\n",
    "    return title \n",
    "\n",
    "def intro(soup):\n",
    "    # In this part we get the INTRO of the movie\n",
    "    \n",
    "    try: # check if intro exists in the TSV file\n",
    "        sec = soup.findAll('p')[0]\n",
    "        if sec == soup.find(\"p\", class_=\"mw-empty-elt\"):\n",
    "            section_intro = soup.findAll('p')[1]\n",
    "        else:\n",
    "            section_intro = sec\n",
    "        nextNode = section_intro\n",
    "        intro = []\n",
    "        intro.append(nextNode.text)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            nextNode = nextNode.find_next_sibling()\n",
    "            if nextNode and nextNode.name == 'p': # looks for the next sibling until it's a paragraph\n",
    "                intro.append(nextNode.text) \n",
    "\n",
    "            else: # if there isn't a paragraph then break\n",
    "                break          \n",
    "        intro_s = \"\" # we create an empty string where we can append every element of INTRO\n",
    "\n",
    "        for ele in intro: \n",
    "            intro_s += ele\n",
    "            \n",
    "        return intro_s\n",
    "    \n",
    "    except IndexError: # Return NONE if INTRO doesn't exists\n",
    "        intro_s = None\n",
    "        return intro_s\n",
    "\n",
    "def plot (soup):\n",
    "    try:    # check if PLOT exists in the TSV file\n",
    "        # In this part we get the PLOT of the movie\n",
    "        sec = soup.findAll('h2')[0]\n",
    "        if sec.text == 'Contents' or sec.text == 'Cast': # this if is used because some times the dataframe's first row is empty\n",
    "            section_plot = soup.findAll('h2')[1]         # plus sometimes the first heading it finds is Content or Cast\n",
    "            if section_plot.text == 'Cast':\n",
    "                section_plot = soup.findAll('h2')[1] \n",
    "        else:\n",
    "            section_plot = sec\n",
    "        nextNode = section_plot.find_next_sibling('p')\n",
    "\n",
    "        plot = []\n",
    "       \n",
    "        while True:\n",
    "\n",
    "            if nextNode and nextNode.name == 'p': # looks for the next sibling until it's a paragraph\n",
    "                plot.append(nextNode.text)\n",
    "                nextNode = nextNode.find_next_sibling()\n",
    "            else:\n",
    "                break          \n",
    "        plot_s = \"\" # we create an empty string where we can append every element of PLOT\n",
    "\n",
    "        for ele in plot: \n",
    "            plot_s += ele\n",
    "        return plot_s\n",
    "    \n",
    "    except IndexError: # Return NONE if PLOT doesn't exists\n",
    "        plot_s = None\n",
    "        return plot_s\n",
    "\n",
    "def infobox(soup):\n",
    "    \n",
    "    try:\n",
    "        table = soup.find('table', class_='infobox vevent')\n",
    "        nextNode = table\n",
    "        table2 = table.find_all('tr')\n",
    "\n",
    "        dic={}\n",
    "        for th in table2[1:]:\n",
    "            if th.find('th'):            \n",
    "                dic[th.find('th').text] = th.find('td').get_text(strip=True, separator='|').split('|')\n",
    "\n",
    "        standard_dic = {\n",
    "        \"Directed by\" : \"\",\n",
    "        \"Produced by\": \"\",\n",
    "        \"Written by\": \"\",\n",
    "        \"Starring\": \"\",\n",
    "        \"Music by\": \"\", \n",
    "        \"Release date\": \"\",\n",
    "        \"Running time\": \"\",\n",
    "        \"Country\": \"\",\n",
    "        \"Language\": \"\",\n",
    "        \"Budget\": \"\"} \n",
    "\n",
    "        # In this part we check if the keys of the infobox are the same as the ones requested   \n",
    "        shared_items = {k: dic[k] for k in dic.keys() & standard_dic.keys()}\n",
    "\n",
    "\n",
    "\n",
    "        # We transform the list into strings\n",
    "        for k, v in shared_items.items():\n",
    "            shared_items[k] = \", \".join(v)\n",
    "\n",
    "        # Difference, we would like to find the missing INFO of this movie\n",
    "        value = { k : standard_dic[k] for k in set(standard_dic) - set(dic) }\n",
    "\n",
    "        # Replace missing INFO with NaN\n",
    "        value = {k: None if not v else v for k, v in value.items() }\n",
    "\n",
    "\n",
    "        # Let's combine these two dictionaries\n",
    "        final = dict(shared_items, **value)\n",
    "\n",
    "        return final\n",
    "    \n",
    "    except AttributeError:\n",
    "        final = {\n",
    "        \"Directed by\" : None,\n",
    "        \"Produced by\": None,\n",
    "        \"Written by\": None,\n",
    "        \"Starring\": None,\n",
    "        \"Music by\": None, \n",
    "        \"Release date\": None,\n",
    "        \"Running time\": None,\n",
    "        \"Country\": None,\n",
    "        \"Language\": None,\n",
    "        \"Budget\": None} \n",
    "        \n",
    "        \n",
    "        return final    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided to transform everything in a dictionary at first, appending the urls of every film as well even if it wasn't reqeusted explicitly. We thought it was the fastest way of retrieving this information that we should output in the search engines ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"C:\\Users\\loren\\Downloads\\HW3\\Movie1\"\n",
    " \n",
    "\n",
    "for i in tqdm_notebook(range (len(os.listdir(dir_path)))):\n",
    "    file_name = os.path.join(dir_path, \"article{}.html\".format(i))\n",
    "    with open(file_name, encoding=\"utf8\") as html_file:\n",
    "\n",
    "        soup = BeautifulSoup(html_file)\n",
    "        t = title(soup)\n",
    "        k = intro(soup)\n",
    "        p = plot(soup)\n",
    "        \n",
    "        \n",
    "        canonical_link = soup.find_all(\"link\",{\"rel\" : \"canonical\"})\n",
    "        url = canonical_link[0].get('href')\n",
    "\n",
    "            \n",
    "        # Write TSV file for each movie, we create a unique dictionary\n",
    "        dic_title = {'Title' : t}\n",
    "        dic_intro = {'Intro' : k}\n",
    "        dic_plot  = {'Plot' : p}\n",
    "        dic_url = {'Url' : url}\n",
    "        dic_infobox = infobox(soup)\n",
    "        \n",
    "        temp = dict(dic_title, **dic_intro) \n",
    "        temp2 = dict(temp, **dic_plot)\n",
    "        temp3 = dict(temp2, **dic_url)\n",
    "        final = dict(temp3, **dic_infobox) # it's the unique dictionary we were talking of before\n",
    "        \n",
    "        with open(r'TSV\\article_{}.tsv'.format(i), 'wt', encoding=\"utf8\") as out_file:\n",
    "            tsv_writer = csv.DictWriter(out_file, final.keys(), delimiter ='\\t')\n",
    "            tsv_writer.writeheader()\n",
    "            tsv_writer.writerow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary and inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_file(dic, file): #defining a function to save files right away\n",
    "    f = open('{}.txt'.format(file), 'w',encoding=\"utf8\") #open a new file in write mode (empty)\n",
    "    f.write(str(dic)) #write in it what we need (it will always be a dictionary, hence 'dic')\n",
    "    f.close() #closing it\n",
    "    \n",
    "def information(data_frame):\n",
    "    col = []\n",
    "    #data_frame.drop(df.columns.difference(['Intro', 'Plot']), 1, inplace=True)\n",
    "    \n",
    "    for column in df:\n",
    "        \n",
    "        # Intro and Plot\n",
    "        if column == 'Plot' or column == 'Intro':\n",
    "            if pd.isnull(data_frame[column][0]):     \n",
    "\n",
    "                try:\n",
    "                    info = str(data_frame[column][1])\n",
    "                except:\n",
    "                    print(filename, \"Intro\")\n",
    "                    return col,('Continue')\n",
    "            else :\n",
    "                info = str(data_frame[column][0])\n",
    "                \n",
    "            col.append(info.replace('\\n',' '))\n",
    "        \n",
    "        else:\n",
    "            # Infobox \n",
    "            if pd.isnull(df[column][0]):      \n",
    "\n",
    "                try:\n",
    "                    infobox = str(df[column][1])\n",
    "                except:\n",
    "                    infobox = ''\n",
    "            else :\n",
    "                infobox = str(df[column][0])\n",
    "\n",
    "            col.append(infobox.replace('\\n',' '))\n",
    "\n",
    "    \n",
    "    \n",
    "    return col, ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start to build up the indexes #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentlist = {}  #to keep track of the words in all files. -->  e.g: {doc_i : ['love, 'movie'.....]}\n",
    "vocabulary = {} #to keep track of all the pre-processed terms and their ids. --> e.g (term_ids) = {'love':3, 'movie':2}\n",
    "inverted_index = {}  #Is the inverted index. term_id as the key and name of the documents as a list of their values. --> e.g: {1: [doc_1, doc_5, ....], 2: [doc_2, doc_4, ....]}\n",
    "word_index = 0 #this is used to give the id to the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"C:\\Users\\Luca\\Desktop\\-\\Università\\Magistrale\\Primo anno\\Primo semestre\\ADM\\Homeworks\\Homework #3\\TSV\"\n",
    " \n",
    "for i in tqdm_notebook(range (len(os.listdir(dir_path)))):\n",
    "    filename = os.path.join(dir_path, \"article_{}.tsv\".format(i))\n",
    "    \n",
    "    # Creating a dataframe for each movie\n",
    "    df = pd.read_csv(filename, sep='\\t', encoding  = 'utf-8')\n",
    "    doc = 'article_{}.tsv'.format(i)\n",
    "    \n",
    "    col = []\n",
    "    col, message = information(df)  \n",
    "    if message == 'Continue':\n",
    "        continue\n",
    "    elif message == 'Pass':\n",
    "        pass\n",
    "     \n",
    "\n",
    "    # Step 2 Taking all the info\n",
    "    \n",
    "    # Column 0, 1 and 2 are respectively : Title, Intro and Plot\n",
    "    # The rest of the columns are part of the infobox\n",
    "    to_tokenize = col[0]+' '+col[1]+' '+col[2]+' '+col[3]+' '+col[4]+' '+col[5]+' '+col[6]+' '+col[7]+' '+col[8]+' '+col[9]+' '+col[10]+' '+col[11]+' '+col[12]+' '+col[13]\n",
    "    tokens = nltk.word_tokenize(to_tokenize) #tokenization\n",
    "    filtered_words = [nltk.stem.PorterStemmer().stem(word) #removing stopwords, special characters, stemming\n",
    "                                for word in tokens if word not in nltk.corpus.stopwords.words('english') and word not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·']\n",
    "    \n",
    "    # Step 3\n",
    "    # Creating a document list: for every document we will have the tokenized words \n",
    "    documentlist[doc] = filtered_words \n",
    "    for w in filtered_words: \n",
    "        \n",
    "        # VOCABULARY\n",
    "        if w not in vocabulary: \n",
    "            word_index += 1    \n",
    "            vocabulary[w] = word_index  \n",
    "        \n",
    "        # INVERTED INDEX\n",
    "        if vocabulary[w] not in inverted_index: \n",
    "            temp = [] \n",
    "            temp.append(doc)  \n",
    "            inverted_index[vocabulary[w]] = temp \n",
    "        \n",
    "        # If the key exists, append the document's name\n",
    "        elif doc not in inverted_index[vocabulary[w]]: \n",
    "            inverted_index[vocabulary[w]].append(doc)  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index with TD*IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docpaths = r\"C:\\Users\\Luca\\Desktop\\-\\Università\\Magistrale\\Primo anno\\Primo semestre\\ADM\\Homeworks\\Homework #3\\TSV\"\n",
    "\n",
    "vocabulary = open('vocabulary.txt', 'r', encoding = 'utf-8')\n",
    "vocabulary = eval(vocabulary.read()) \n",
    "\n",
    "inverted_index = open('inverted_index.txt', 'r', encoding = 'utf-8') \n",
    "inverted_index = eval(inverted_index.read())\n",
    "\n",
    "documentlist = open('documentlist.txt', 'r', encoding = 'utf-8')\n",
    "documentlist = eval(documentlist.read())\n",
    "\n",
    "new_inverted_index = {}\n",
    "for key,doc in tqdm_notebook(documentlist.items()): #taking the keys (doc_i) and the values \n",
    "    \n",
    "    for w in doc: \n",
    "        score = tfidf(w,doc) \n",
    "        w_index = (key,score) \n",
    "        if vocabulary[w] not in new_inverted_index: \n",
    "            temp = [] \n",
    "            temp.append(w_index) \n",
    "            new_inverted_index[vocabulary[w]] = temp \n",
    "        elif doc not in new_inverted_index[vocabulary[w]]: \n",
    "            new_inverted_index[vocabulary[w]].append(w_index) \n",
    "\n",
    "# Removing duplicates\n",
    "for key in new_inverted_index:\n",
    "    new_inverted_index[key] = list(set(new_inverted_index[key]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "save_dict_to_file(new_inverted_index,\"inverted_index_tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine n. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this moment, we narrow our interest on the intro and plot of each document. It means that the first Search Engine will evaluate queries with respect to the aforementioned information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " disney \n"
     ]
    }
   ],
   "source": [
    "# Taking the user's query\n",
    "query = input() \n",
    "\n",
    "# Tokenizing the query\n",
    "tokens = nltk.word_tokenize(query) \n",
    "query_tokens = [nltk.stem.PorterStemmer().stem(token) # Removing stopwords, special characters, stemming\n",
    "                for token in tokens if token not in stopwords.words('english') if token not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·']\n",
    "\n",
    "# Let's open vocabulary and inverted index in read mode\n",
    "vocabulary = open('vocabulary.txt', 'r', encoding = 'utf-8') \n",
    "vocabulary = eval(vocabulary.read()) \n",
    "inverted_index = open('inverted_index.txt', 'r', encoding = 'utf-8')  \n",
    "inverted_index = eval(inverted_index.read())   \n",
    "\n",
    "# Taking the term_ids of the query's terms (returns a list of terms_ids) from Vocabulary\n",
    "term_ids = [vocabulary[token] for token in query_tokens if token in vocabulary] \n",
    "\n",
    "# Returns the documents in the inverted idx for that have that same ID\n",
    "search_results = [inverted_index[i] for i in term_ids if i in inverted_index] \n",
    "\n",
    "# Removing duplicates and preparing for intersection (conjunction)\n",
    "new_list = [set(list_) for list_ in search_results]\n",
    "\n",
    "# Returns the documents that have all the words of the query\n",
    "intersect = set.intersection(*new_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Title\",\"Intro\", 'Url']\n",
    "search1 = pd.DataFrame(columns = col_names) \n",
    "for j, doc in enumerate(intersect): \n",
    "    with open(r'TSVFile\\{}'.format(doc), 'r',encoding=\"utf-8\") as file: \n",
    "        df = pd.read_csv(file, sep='\\t', encoding  = 'utf-8')\n",
    "        # Drop every column that is not TITLE, INTRO and URL\n",
    "        df.drop(df.columns.difference(['Title','Intro', 'Url']), 1, inplace=True)\n",
    "        search1 = pd.concat([df,search1], axis = 0, ignore_index=True, sort = False)\n",
    "\n",
    "# Making the rows more readabale    \n",
    "printmd(\"The intitial query was: ***{}***\".format(query))       \n",
    "pd.set_option('max_colwidth',500) \n",
    "display(search1.style.set_table_styles([ {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine n.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Create Inverted Index containing TF*IDF score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF*IDF algorithm is used to weigh a keyword in any content and assign the importance to that keyword based on the number of times it appears in the document. \n",
    "* The __TF__ (term frequency) of a word is the frequency of a word (i.e. number of times it appears) in a document. When you know it, you’re able to see if you’re using a term too much or too little.\n",
    "* The __IDF__ (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized value tf. 'Term frequency' divided by 'document length'. In this way the bias of having a long document doesn't count\n",
    "def tf(word, document):\n",
    "    return document.count(word) / len(document) \n",
    "\n",
    "# Number of documents with the same word\n",
    "def document_frequency(word):\n",
    "    if word in vocabulary:\n",
    "        term_id = vocabulary[word]\n",
    "    return len(inverted_index[term_id])\n",
    "\n",
    "# IDF(word) = log(Total Number Of Documents / Number Of Documents containing the certain term (word))\n",
    "def idf(word):\n",
    "    return math.log(len(docpaths) / document_frequency(word))\n",
    "\n",
    "def tfidf(word, document):\n",
    "    return tf(word, document) * idf(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's open all the .txt files we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the query\n",
    "query = input() \n",
    "tokens = nltk.word_tokenize(query) \n",
    "query_tokens = [nltk.stem.PorterStemmer().stem(token)\n",
    "                for token in tokens if token not in stopwords.words('english') if token not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for the query\n",
    "tfidf_query_array = [] \n",
    "for w in query_tokens:  \n",
    "    score = tf(w, query_tokens) * idf(w) \n",
    "    tfidf_query_array.append(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the new inverted index  \n",
    "inverted_index_tfidf = open('inverted_index_tfidf.txt', 'r', encoding = 'utf-8')  \n",
    "inverted_index_tfidf = eval(inverted_index_tfidf.read())\n",
    "\n",
    "# Terms Id's of the term/s in the query\n",
    "term_ids = [vocabulary[token] for token in query_tokens if token in vocabulary] \n",
    "# List of tuples (doc_id, score) for each id\n",
    "search_results = [inverted_index_tfidf[idx] for idx in term_ids if idx in inverted_index_tfidf] \n",
    "\n",
    "# Cosine similarity\n",
    "cos_arrays = {} \n",
    "for list_ in search_results: \n",
    "    for tuple_ in list_: \n",
    "        if tuple_[0] not in cos_arrays: \n",
    "            temp = []\n",
    "            temp.append(tuple_[1]) \n",
    "            cos_arrays[tuple_[0]] = temp \n",
    "        else:\n",
    "            cos_arrays[tuple_[0]].append(tuple_[1]) \n",
    "\n",
    "# Take the documents that have all of the words\n",
    "final = [(key,cos_arrays[key]) for key in cos_arrays if len(cos_arrays[key]) == len(term_ids)] \n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim = {} #dic for {doc_i : similarity_score, ...}\n",
    "for tuple_ in final: #take the tuple in final (the values are the list of the tf/idf scores)\n",
    "    # Compute the the cosine similarity\n",
    "    sim = 1 - (scipy.spatial.distance.cosine(np.array(tfidf_query_array) , np.array(tuple_[1]))) \n",
    "    # key = doc_id, value = similarity\n",
    "    doc_sim[tuple_[0]] =  sim \n",
    "print(doc_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that has a tuple ordered by the smallest to the largest\n",
    "heap = [(-value, key) for key, value in doc_sim.items()] \n",
    "# Taking the largest\n",
    "largest = heapq.nsmallest(10, heap) \n",
    "# Ordering keys and values\n",
    "largest = [(key, -value) for value, key in largest] \n",
    "\n",
    "#Printing the search results\n",
    "col_names = [\"Title\",\"Intro\", \"Similarity\", \"Url\"]\n",
    "score = pd.DataFrame(columns = col_names) \n",
    "\n",
    "# We use reversed(largest) so that we can add values from the smallest to the greatest\n",
    "for j, doc in enumerate(reversed(largest)): \n",
    "    with open(r'TSV\\{}'.format(doc[0]), 'r',encoding=\"utf-8\") as file: \n",
    "        df = pd.read_csv(file, sep='\\t', encoding  = 'utf-8')\n",
    "        df.drop(df.columns.difference(['Title','Intro', 'Url', 'Similarity']), 1, inplace=True)\n",
    "        df.loc[0, 'Similarity'] = doc[1]\n",
    "        score = pd.concat([df,score], axis = 0, ignore_index=True, sort = False)\n",
    "\n",
    "# Output\n",
    "printmd(\"The intitial query was: ***{}***\".format(query))       \n",
    "pd.set_option('max_colwidth',500)\n",
    "score.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine n. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the weight of RUNTIME and RELEASE DATE\n",
    "def importance(dataframe):\n",
    "    \n",
    "    if user == 1:\n",
    "        score_runtime = 0.7\n",
    "        score_release = 0.3\n",
    "    elif user == 2:\n",
    "        score_runtime = 0.3\n",
    "        score_release = 0.7\n",
    "    else:\n",
    "        score_runtime = 0.5\n",
    "        score_release = 0.5 \n",
    "    return score_runtime, score_release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you pick the language. The main query will be based on this.\n",
    "\n",
    "Second, you pick if the **lenght of the movie** is more relevant to you than the **release date**, the other way around or if it's the same.\n",
    "\n",
    "\n",
    "The score is calculated based on the aswer answer:\n",
    "* If someone answers 1 ---> weight is 0.7 for runtime and 0.3 for release date\n",
    "* If someone answers 2 ---> weight is 0.3 for runtime and 0.7 for release dates \n",
    "* If someone answers 3 ---> weight is 0.5 for runtime and 0.5 fo release date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our weights we can compute the score:\n",
    "\n",
    "$$score = (runtime\\_score  {1 \\over \\ \\mid runtime-user\\_runtime \\mid +1 }) + (release\\_score  {1 \\over \\ \\mid release-user\\_release \\mid +1 })$$\n",
    "\n",
    "\n",
    "Where:\n",
    "* runtime = runtime in TSV files\n",
    "* user_runtime = runtime that the user inputs\n",
    "* release = release in TSV files\n",
    "* user_release = release that the user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the language of the movie you would like to watch\n",
    "language = input('What language would you like to see the movie in? ') \n",
    "\n",
    "# Select the runtime and the release year\n",
    "user_runtime = int(input('Insert the length of the movie you would like to watch(in minutes): ')) \n",
    "user_release = int(input('Insert the release year of the movie you would like to watch: '))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = int(input(\"Now, what is more important to you? Is it the running time or the release date?\\nAswer 1 for the first, 2 for the second or 3 for both: \"))\n",
    "our_score = {}\n",
    "# Just to check how many documents have a Null running time or release date\n",
    "error = []\n",
    "\n",
    "# Iterating through the intersection list created in the 1st search engine\n",
    "for j, doc in enumerate(intersect): \n",
    "    score = 0 \n",
    "    df = pd.read_csv(r'TSV\\{}'.format(doc), sep='\\t', encoding  = 'utf-8')\n",
    "    if pd.isnull(df['Release date'][0]) == True or pd.isnull(df['Running time'][0]) == True  :\n",
    "        print(doc)\n",
    "        error.append(doc) \n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        # Checking if the cell of the TSV is already an INT or a STRING\n",
    "        if type(df['Running time'][0]) != np.int64:\n",
    "            df['Running time'] = re.sub(\"[^0-9]\", \"\",df['Running time'][0])\n",
    "        \n",
    "        if type(df['Release date'][0]) != np.int64:\n",
    "            # Takes the first four numbers of the TSV cell\n",
    "            s = set(re.findall(r\"\\b\\d{4}\\b\", df['Release date'][0]))\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            # Taking only the first year of the set\n",
    "            df['Release date']  = next(iter(s))\n",
    "        \n",
    "        # Is the language we picked the same to the movie we are analyzing?\n",
    "        if language == df['Language'][0]: \n",
    "\n",
    "            score_runtime, score_release = importance(user)\n",
    "\n",
    "\n",
    "            # If the runtime is included between a range of +- 10 minutes the score increments  \n",
    "            # or the release date is included between a range of +- 5 years.\n",
    "            if ((abs(user_runtime) <= abs(int(df['Running time'][0]))+10) and (abs(user_runtime)>= abs(int(df['Running time'][0]))-10)) or ((abs(user_release) <= abs(int(df['Release date'][0]))+5) and (abs(user_release)>= abs(int(df['Release date'][0]))-5)):\n",
    "                score = (score_release*1/(abs(int(df['Release date'][0])- user_release)+1)) + (score_runtime*1/(abs(int(df['Running time'][0])- user_runtime)+1))\n",
    "            \n",
    "            # If its outside the range, we apply a penalty = score * 0.5\n",
    "            else: \n",
    "                score = (score_release*1/(abs(int(df['Release date'][0])- user_release)+1)) + (score_runtime*1/(abs(int(df['Running time'][0])- user_runtime)+1))*0.5\n",
    "        else:\n",
    "            \n",
    "            continue\n",
    "\n",
    "    our_score[doc] = score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = [(-value, key) for key, value in our_score.items()] \n",
    "largest = heapq.nsmallest(10, heap)\n",
    "largest = [(key, -value) for value, key in largest]\n",
    "# Printing the search results\n",
    "col_names = [\"Title\",\"Intro\", \"Url\"]\n",
    "our_score = pd.DataFrame(columns = col_names) #creating an empty df with the list cerated before\n",
    "\n",
    "# We use reversed(largest) so that we can add values from the smallest to the greatest\n",
    "for j, doc in enumerate(reversed(largest)): \n",
    "    with open(r'TSV\\{}'.format(doc[0]), 'r',encoding=\"utf-8\") as file: #opening the files in read mode\n",
    "        df_final = pd.read_csv(file, sep='\\t', encoding  = 'utf-8')\n",
    "        df_final.drop(df.columns.difference(['Title','Intro', 'Url']), 1, inplace=True)\n",
    "        percentage = doc[1]\n",
    "        df_final['Score'] = percentage\n",
    "        our_score = pd.concat([df_final,our_score], axis = 0, ignore_index=True, sort = False)\n",
    "\n",
    "# Output\n",
    "#printmd(\"The intitial query was: ***{}***\".format(query))       \n",
    "pd.set_option('max_colwidth',500)\n",
    "our_score.style.background_gradient(cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
